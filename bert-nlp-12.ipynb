{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shivamsourav2002/bert-nlp-12?scriptVersionId=172656511\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# EXPERIMENT NO 11\nEXTEND THE NER EXPERIMENT TO INCLUDE NAMED ENTITY LINKING ASSOCIATED RECOGNIZED ENTITIES WITH THEIR CORRESPONDING DATABASE ","metadata":{}},{"cell_type":"markdown","source":"# EXPERIMENT NO 12\nUse pre-trained BERT models for various NLP tasks like question answering , sentiment analysis and text classification. Fine tune BERT for a specific task and compare its performance with traditional models.","metadata":{}},{"cell_type":"code","source":"import torch \nfrom transformers import BertTokenizer,BertForSequenceClassification\nfrom torch.nn import functional as F\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n\nsentiment_labels = {0:'Negative',1:'Neutral',2:'Positive'}\ndef predict_sentiment(text):\n    inputs = tokenizer(text,return_tensors='pt',padding=True,truncation=True)\n    outputs = model(**inputs)\n    logits = outputs.logits\n    probabilities = F.softmax(logits,dim=1)\n    \n    predicted_label = torch.argmax(probabilities,dim=1).item()\n    predicted_sentiment = sentiment_labels[predicted_label]\n    return predicted_sentiment,probabilities\n\ntext =  \"\"\"I didn't enjoyed the movie, it was very horrible\"\"\"\npredicted_sentiment,probabilities = predict_sentiment(text)\nprint(f\"Predicted sentiment : {predicted_sentiment} with confidence of {probabilities}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T09:21:28.914918Z","iopub.execute_input":"2024-04-18T09:21:28.915587Z","iopub.status.idle":"2024-04-18T09:21:29.720659Z","shell.execute_reply.started":"2024-04-18T09:21:28.915553Z","shell.execute_reply":"2024-04-18T09:21:29.719698Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Predicted sentiment : Negative with confidence of tensor([[0.6721, 0.3279]], grad_fn=<SoftmaxBackward0>)%\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nsa = pipeline('text-classification', model=model,tokenizer=tokenizer)\nsa(\"\"\"I didn't enjoyed the movie, it was very horrible\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T09:29:57.686403Z","iopub.execute_input":"2024-04-18T09:29:57.6868Z","iopub.status.idle":"2024-04-18T09:29:57.768974Z","shell.execute_reply.started":"2024-04-18T09:29:57.68677Z","shell.execute_reply":"2024-04-18T09:29:57.768022Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'label': 'LABEL_0', 'score': 0.6721088886260986}]"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/tf-idf-data/dataset.csv')\n\n# Encode the text data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nencoded_data = tokenizer.batch_encode_plus(\n    data['text'].values,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    padding='max_length',\n    max_length=512,\n    return_tensors='pt'\n)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(data['label'].values)\nlabels = torch.tensor(labels)\n\n# Create input tensors\ninput_ids = encoded_data['input_ids']\nattention_masks = encoded_data['attention_mask']\n\n# Create the dataset and dataloader\ndataset = TensorDataset(input_ids, attention_masks, labels)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Load the pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(set(data['label'])),  # Number of unique labels\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Set the device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Define the optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for batch in dataloader:\n        # Unpack the batch\n        b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask, labels=b_labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n\n# Evaluation\nmodel.eval()\neval_accuracy = 0\neval_data = TensorDataset(input_ids, attention_masks, labels)\neval_dataloader = DataLoader(eval_data, batch_size=16)\n\nfor batch in eval_dataloader:\n    b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n\n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)[0]\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    eval_accuracy += (logits.argmax(axis=-1) == label_ids).mean()\n\neval_accuracy /= len(eval_dataloader)\nprint(f'Evaluation Accuracy: {eval_accuracy*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T09:38:52.157671Z","iopub.execute_input":"2024-04-18T09:38:52.158417Z","iopub.status.idle":"2024-04-18T09:43:55.42696Z","shell.execute_reply.started":"2024-04-18T09:38:52.158387Z","shell.execute_reply":"2024-04-18T09:43:55.425814Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 0.6914660930633545\nEpoch 2/3, Loss: 0.7006637454032898\nEpoch 3/3, Loss: 0.6910300254821777\nEvaluation Accuracy: 52.08%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to perform inference on a single input string\ndef predict(input_text):\n    # Encode the input text\n    encoded_input = tokenizer.encode_plus(\n        input_text,\n        add_special_tokens=True,\n        return_attention_mask=True,\n        padding='max_length',\n        max_length=512,\n        return_tensors='pt'\n    )\n\n    input_ids = encoded_input['input_ids'].to(device)\n    attention_mask = encoded_input['attention_mask'].to(device)\n\n    # Forward pass\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    # Get the predicted label\n    predicted_label_idx = logits.argmax().item()\n    predicted_label = label_encoder.inverse_transform([predicted_label_idx])\n\n    return predicted_label[0]\n# Get input text from the user\ninput_text = input(\"Enter your text: \")\n\n# Perform inference and print the predicted label\npredicted_label = predict(input_text)\nprint(f\"Predicted label: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:01:12.001054Z","iopub.execute_input":"2024-04-18T10:01:12.001427Z","iopub.status.idle":"2024-04-18T10:01:19.796444Z","shell.execute_reply.started":"2024-04-18T10:01:12.001397Z","shell.execute_reply":"2024-04-18T10:01:19.795488Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your text:  The product which i purchased from Amazon is very bad and not useful in daily life.\n"},{"name":"stdout","text":"Predicted label: negative\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}