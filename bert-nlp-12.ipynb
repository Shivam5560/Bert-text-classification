{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shivamsourav2002/bert-nlp-12?scriptVersionId=172662896\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# EXPERIMENT NO 11\nEXTEND THE NER EXPERIMENT TO INCLUDE NAMED ENTITY LINKING ASSOCIATED RECOGNIZED ENTITIES WITH THEIR CORRESPONDING DATABASE ","metadata":{}},{"cell_type":"markdown","source":"# EXPERIMENT NO 12\nUse pre-trained BERT models for various NLP tasks like question answering , sentiment analysis and text classification. Fine tune BERT for a specific task and compare its performance with traditional models.","metadata":{}},{"cell_type":"code","source":"import torch \nfrom transformers import BertTokenizer,BertForSequenceClassification\nfrom torch.nn import functional as F\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name)\n\nsentiment_labels = {0:'Negative',1:'Neutral',2:'Positive'}\ndef predict_sentiment(text):\n    inputs = tokenizer(text,return_tensors='pt',padding=True,truncation=True)\n    outputs = model(**inputs)\n    logits = outputs.logits\n    probabilities = F.softmax(logits,dim=1)\n    \n    predicted_label = torch.argmax(probabilities,dim=1).item()\n    predicted_sentiment = sentiment_labels[predicted_label]\n    return predicted_sentiment,probabilities\n\ntext =  \"\"\"I didn't enjoyed the movie, it was very horrible\"\"\"\npredicted_sentiment,probabilities = predict_sentiment(text)\nprint(f\"Predicted sentiment : {predicted_sentiment} with confidence of {probabilities}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:31:53.289556Z","iopub.execute_input":"2024-04-18T10:31:53.290478Z","iopub.status.idle":"2024-04-18T10:32:09.011298Z","shell.execute_reply.started":"2024-04-18T10:31:53.290443Z","shell.execute_reply":"2024-04-18T10:32:09.01016Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dbd20ba91c940ca872eebd4554211d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce76d186a13f4fbf8ae9e35cbb0d55a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe649977253c49fc98bccc2d16a91bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1041e19e58e54e82aafeb2749ba6b8bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524e5aed99f9491e9651819a34f24cab"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Predicted sentiment : Neutral with confidence of tensor([[0.3082, 0.6918]], grad_fn=<SoftmaxBackward0>)%\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nsa = pipeline('text-classification', model=model,tokenizer=tokenizer)\nsa(\"\"\"The movie was very Good , I liked it too much.\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:32:09.013549Z","iopub.execute_input":"2024-04-18T10:32:09.014822Z","iopub.status.idle":"2024-04-18T10:32:26.671424Z","shell.execute_reply.started":"2024-04-18T10:32:09.01479Z","shell.execute_reply":"2024-04-18T10:32:26.670462Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-04-18 10:32:12.418024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-18 10:32:12.418129: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-18 10:32:12.696894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[{'label': 'LABEL_1', 'score': 0.6913214325904846}]"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv')\ndata = data[:int(data.shape[0]*0.2)]\ntest = data[int(data.shape[0]*0.2):]\n\n\n# Encode the text data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nencoded_data = tokenizer.batch_encode_plus(\n    data['text'].values,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    padding='max_length',\n    max_length=512,\n    return_tensors='pt',\n    truncation=True\n)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(data['label'].values)\nlabels = torch.tensor(labels)\n\n# Create input tensors\ninput_ids = encoded_data['input_ids']\nattention_masks = encoded_data['attention_mask']\n\n# Create the dataset and dataloader\ndataset = TensorDataset(input_ids, attention_masks, labels)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Load the pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(set(data['label'])),  # Number of unique labels\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Set the device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Define the optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for batch in dataloader:\n        # Unpack the batch\n        b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Forward pass\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask, labels=b_labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n\n# Evaluation\nmodel.eval()\neval_accuracy = 0\neval_data = TensorDataset(input_ids, attention_masks, labels)\neval_dataloader = DataLoader(eval_data, batch_size=16)\n\nfor batch in eval_dataloader:\n    b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n\n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_mask)[0]\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    eval_accuracy += (logits.argmax(axis=-1) == label_ids).mean()\n\neval_accuracy /= len(eval_dataloader)\nprint(f'Evaluation Accuracy: {eval_accuracy*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:32:26.6728Z","iopub.execute_input":"2024-04-18T10:32:26.673397Z","iopub.status.idle":"2024-04-18T10:37:30.227788Z","shell.execute_reply.started":"2024-04-18T10:32:26.67337Z","shell.execute_reply":"2024-04-18T10:37:30.226814Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 0.08677767217159271\nEpoch 2/3, Loss: 0.023762719705700874\nEpoch 3/3, Loss: 0.00340729346498847\nEvaluation Accuracy: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"text = test.iloc[0]['text']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:38:08.026278Z","iopub.execute_input":"2024-04-18T10:38:08.026652Z","iopub.status.idle":"2024-04-18T10:38:08.031424Z","shell.execute_reply.started":"2024-04-18T10:38:08.026624Z","shell.execute_reply":"2024-04-18T10:38:08.030459Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:38:17.15244Z","iopub.execute_input":"2024-04-18T10:38:17.153235Z","iopub.status.idle":"2024-04-18T10:38:17.159343Z","shell.execute_reply.started":"2024-04-18T10:38:17.153203Z","shell.execute_reply":"2024-04-18T10:38:17.158356Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'Subject: calpine daily gas nomination\\r\\n>\\r\\nricky a . archer\\r\\nfuel supply\\r\\n700 louisiana , suite 2700\\r\\nhouston , texas 77002\\r\\n713 - 830 - 8659 direct\\r\\n713 - 830 - 8722 fax\\r\\n- calpine daily gas nomination 1 . doc'"},"metadata":{}}]},{"cell_type":"code","source":"# Function to perform inference on a single input string\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ndef predict(input_text):\n    # Encode the input text\n    encoded_input = tokenizer.encode_plus(\n        input_text,\n        add_special_tokens=True,\n        return_attention_mask=True,\n        padding='max_length',\n        max_length=512,\n        return_tensors='pt'\n    )\n\n    input_ids = encoded_input['input_ids'].to(device)\n    attention_mask = encoded_input['attention_mask'].to(device)\n\n    # Forward pass\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    # Get the predicted label\n    predicted_label_idx = logits.argmax().item()\n    predicted_label = label_encoder.inverse_transform([predicted_label_idx])\n\n    return predicted_label[0]\n# Get input text from the user\ninput_text = input(\"Enter your text: \")\n\n# Perform inference and print the predicted label\npredicted_label = predict(input_text)\nprint(f\"Predicted label: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T10:38:22.094364Z","iopub.execute_input":"2024-04-18T10:38:22.09523Z","iopub.status.idle":"2024-04-18T10:38:24.75663Z","shell.execute_reply.started":"2024-04-18T10:38:22.095198Z","shell.execute_reply":"2024-04-18T10:38:24.755584Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your text:  Subject: calpine daily gas nomination\\r\\n>\\r\\nricky a . archer\\r\\nfuel supply\\r\\n700 louisiana , suite 2700\\r\\nhouston , texas 77002\\r\\n713 - 830 - 8659 direct\\r\\n713 - 830 - 8722 fax\\r\\n- calpine daily gas nomination 1 . doc\n"},{"name":"stdout","text":"Predicted label: ham\n","output_type":"stream"}]}]}